{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import demoji\n",
    "# demoji.download_codes()\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import seaborn as sns\n",
    "import emoji\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anweshcr7/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anweshcr7/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conda install nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 5.3MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-0.5.4-cp37-none-any.whl size=42177 sha256=422892a1468ae4635f9dd536d951ae845f45ce978e686e49d2985bb15125e65c\n",
      "  Stored in directory: /Users/anweshcr7/Library/Caches/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some additional imports\n",
    "# from autocorrect import speller\n",
    "stopwordsList = stopwords.words('english')\n",
    "# ended up lemmatizing instead of stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(fp):\n",
    "    '''\n",
    "    Loads the dataset file with label-tweet on each line and parses the dataset.\n",
    "    :param fp: filepath of dataset\n",
    "    :return:\n",
    "        corpus: list of tweet strings of each tweet.\n",
    "        y: list of labels\n",
    "    '''\n",
    "    y = []\n",
    "    corpus = []\n",
    "    with open(fp, 'rt') as data_in:\n",
    "        for line in data_in:\n",
    "            if not line.lower().startswith(\"tweet index\"): # discard first line if it contains metadata\n",
    "                line = line.rstrip() # remove trailing whitespace\n",
    "                label = int(line.split(\"\\t\")[1])\n",
    "                tweet = line.split(\"\\t\")[2]\n",
    "                y.append(label)\n",
    "                corpus.append(tweet)\n",
    "\n",
    "    return corpus, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, y_label = parse_dataset('SemEval2018-T3-train-taskA_emoji.txt')\n",
    "data_df = pd.DataFrame(np.array(data).reshape(3834,1), columns = ['tweet'])\n",
    "data_df['label'] = np.array(y_label).reshape(3834,1)\n",
    "data_df.loc[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "\n",
    "    clean_tweet = clean(tweet.lower())\n",
    "    return clean_tweet\n",
    "\n",
    "def preprocess(tweet):\n",
    "#     Remove URL\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "#     remove hashtag (symbols) and punctuations\n",
    "    punctuations = '''!()-![]{};:+'\"\\,<>./?@#$%^&*_~'''\n",
    "    tweet = ''.join([i for i in tweet if not i in punctuations])\n",
    "#     reomve emoji\n",
    "    tweet = demoji.replace(tweet)\n",
    "#     correct spellings if any\n",
    "# Lemmatize over Stemming\n",
    "    tweet = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in tweet.split()])\n",
    "# Gives clean_tweet but now lets tokenize\n",
    "# tokenize and remove stop-words\n",
    "    tweet = word_tokenize(tweet)\n",
    "    return ' '.join([word for word in tweet if word not in stopwordsList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talked'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweet = \"@mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing \"\n",
    "# tweet = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in tweet.split()])\n",
    "# wordnet_lemmatizer.lemmatize('talked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hashtag_emoji(tweet):\n",
    "    tweet = re.sub(r'\\B#','', tweet)\n",
    "    tweet = emoji.demojize(tweet, delimiters=(\"\", \"\"))\n",
    "    return tweet\n",
    "\n",
    "# replace_hashtag_emoji('anwesh #hello ðŸ”¥ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some feature engineering before pre-processing (might be useful!)\n",
    "def preprocess_and_engineer(df):\n",
    "    df['eng_tweet'] = data_df['tweet'].apply(lambda x: replace_hashtag_emoji(x)) #creating a new column\n",
    "#     df['emoji'] = data_df['tweet'].apply(lambda x: demoji.findall(x)) #create a new emoji column\n",
    "    df['clean_tweet'] = df['eng_tweet'].apply(lambda x: preprocess(x))\n",
    "#     del df['tweet']\n",
    "    cols = ['clean_tweet','label']\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_irony = data_df[ data_df['label'] == 1]\n",
    "# train_irony = train_irony['tweet']\n",
    "# train_normal = data_df[ data_df['label'] == 0]\n",
    "# train_normal = train_normal['tweet']\n",
    "\n",
    "# def wordcloud_draw(data, color = 'black'):\n",
    "#     words = ' '.join(data)\n",
    "#     cleaned_word = \" \".join([word for word in words.split()\n",
    "#                             if 'http' not in word\n",
    "#                                 and not word.startswith('@')\n",
    "#                                 and not word.startswith('#')\n",
    "#                                 and word != 'RT'\n",
    "#                             ])\n",
    "#     wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "#                       background_color=color,\n",
    "#                       width=2500,\n",
    "#                       height=2000\n",
    "#                      ).generate(cleaned_word)\n",
    "#     plt.figure(1,figsize=(13, 13))\n",
    "#     plt.imshow(wordcloud)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "# print(\"Positive words\")\n",
    "# wordcloud_draw(train_irony,'white')\n",
    "# print(\"Negative words\")\n",
    "# wordcloud_draw(train_normal)\n",
    "# # Doesn't seem to make a lot of difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sweet United Nations video Just time Christmas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>mrdahl87 We rumored talked Ervs agent Angels a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey Nice see MinnesotaND Winter Weather</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3 episode left Im dying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I cant breathe wa chosen notable quote year an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_tweet  label\n",
       "0  Sweet United Nations video Just time Christmas...      1\n",
       "1  mrdahl87 We rumored talked Ervs agent Angels a...      1\n",
       "2            Hey Nice see MinnesotaND Winter Weather      1\n",
       "3                            3 episode left Im dying      0\n",
       "4  I cant breathe wa chosen notable quote year an...      1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data_df = preprocess_and_engineer(data_df)\n",
    "clean_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Returns a victorizer fitted with the training data\n",
    "def vectorized_features(df):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', sublinear_tf=True, strip_accents='unicode',\n",
    "                                 analyzer='word', norm=None, ngram_range = (1,3))\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(df['clean_tweet'])\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_vectorizer = vectorized_features(clean_data_df)\n",
    "# tweet_vector = tweet_vectorizer.transform(clean_data_df['clean_tweet'])\n",
    "# tweet_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sweet United Nations video Just time Christmas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>mrdahl87 We rumored talked Ervs agent Angels a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey Nice see MinnesotaND Winter Weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3 episode left Im dying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I cant breathe wa chosen notable quote year an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_tweet  label\n",
       "0  Sweet United Nations video Just time Christmas...      0\n",
       "1  mrdahl87 We rumored talked Ervs agent Angels a...      1\n",
       "2            Hey Nice see MinnesotaND Winter Weather      0\n",
       "3                            3 episode left Im dying      0\n",
       "4  I cant breathe wa chosen notable quote year an...      1"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data, test_label = parse_dataset('SemEval2018-T3_gold_test_taskA_emoji.txt')\n",
    "test_df = pd.DataFrame(np.array(test_data).reshape(784,1), columns = ['tweet'])\n",
    "test_df['label'] = np.array(test_label).reshape(784,1)\n",
    "test_df = preprocess_and_engineer(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = tweet_vectorizer.transform(test_df['clean_tweet'])\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a Naive Bayes Implementation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "score = []\n",
    "# kfold = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# for train, test in kfold.split(clean_data_df):\n",
    "#     train_tf_idf = tweet_vectorizer.fit_transform(clean_data_df['clean_tweet'][train])\n",
    "#     gnb.fit(train_tf_idf.todense(), clean_data_df['label'][train])\n",
    "#     test_tf_idf = tweet_vectorizer.transform(clean_data_df['clean_tweet'][test])\n",
    "#     y_pred = gnb.predict(test_tf_idf.todense())\n",
    "#     score.append(metrics.accuracy_score(clean_data_df['label'][test], y_pred))\n",
    "train_tf_idf = tweet_vectorizer.fit_transform(clean_data_df['clean_tweet'])\n",
    "gnb.fit(train_tf_idf.todense(), clean_data_df['label'])\n",
    "\n",
    "# from statistics import mean \n",
    "\n",
    "# print(mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6347    0.5032    0.5613       473\n",
      "           1     0.4254    0.5595    0.4833       311\n",
      "\n",
      "    accuracy                         0.5255       784\n",
      "   macro avg     0.5300    0.5313    0.5223       784\n",
      "weighted avg     0.5517    0.5255    0.5304       784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.predict(test_vector.toarray())\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCM(cm):\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt=\"d\"); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(['Irony', 'Non-Irony']); ax.yaxis.set_ticklabels(['Irony', 'Non-Irony']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "nb_cm = confusion_matrix(y_test, y_pred)\n",
    "plotCM(nb_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
