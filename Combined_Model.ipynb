{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/anweshcr7/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/anweshcr7/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/anweshcr7/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/anweshcr7/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import emoji\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "### Init lists for the analysis\n",
    "\n",
    "HAPPY = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P'\n",
    "    , ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "SAD = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "dt = ['not', 'no', 'for', 'and', 'nor', 'but', 'or', 'so',\n",
    "        'while', 'if', 'only', \n",
    "        'until', 'than', \n",
    "         'as', 'after', 'before',\n",
    "        'by', 'now', 'once',\n",
    "        'when', 'because','in',\n",
    "        'why', 'what', 'which', 'who', \n",
    "         'how', 'where','just', 'both', \n",
    "        'with', 'then']\n",
    "\n",
    "conjunctions = ['for', 'and', 'nor', 'but', 'or', 'yet', 'so',\n",
    "        'though', 'although', 'even though', 'while', 'if', 'only if', 'unless',\n",
    "        'until', 'provided that', 'assuming that', 'even if', 'in case', 'than', 'rather than',\n",
    "        'whether', 'as much as', 'whereas', 'after', 'as long as', 'as soon as', 'before',\n",
    "        'by the time', 'now that', 'once', 'since', 'till', 'until',\n",
    "        'when', 'whenever', 'while', 'because', 'since', 'so that', 'in order',\n",
    "        'why', 'that', 'what', 'whatever', 'which', 'whichever', 'who', 'whoever',\n",
    "        'whom', 'whomever', 'whose', 'how', 'as though', 'as if','where', 'wherever',\n",
    "        'also', 'besides', 'furthermore', 'likewise', 'moreover', 'however', 'nevertheless',\n",
    "        'nonetheless', 'still', 'conversely', 'instead', 'otherwise', 'rather', 'accordingly',\n",
    "        'consequently', 'hence', 'meanwhile', 'then', 'therefore', 'thus']\n",
    "\n",
    "NEGATE = {'ain\\'t', 'aren\\'t', 'cannot', 'can\\'t', 'couldn\\'t', 'daren\\'t', 'didn\\'t', 'doesn\\'t',\n",
    " 'ain\\'t', 'aren\\'t', 'cant', 'couldn\\'t', 'daren\\'t', 'didn\\'t', 'doesn\\'t',\n",
    " \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", 'neither',\n",
    " \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    " \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    " \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    " \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    " \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\",'!'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(fp):\n",
    "    '''\n",
    "    Loads the dataset file with label-tweet on each line and parses the dataset.\n",
    "    :param fp: filepath of dataset\n",
    "    :return:\n",
    "        corpus: list of tweet strings of each tweet.\n",
    "        y: list of labels\n",
    "    '''\n",
    "    y = []\n",
    "    corpus = []\n",
    "    with open(fp, 'rt') as data_in:\n",
    "        for line in data_in:\n",
    "            if not line.lower().startswith(\"tweet index\"): # discard first line if it contains metadata\n",
    "                line = line.rstrip() # remove trailing whitespace\n",
    "                label = int(line.split(\"\\t\")[1])\n",
    "                tweet = line.split(\"\\t\")[2]\n",
    "                y.append(label)\n",
    "                corpus.append(tweet)\n",
    "\n",
    "    return corpus, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"I can't breathe!\" was chosen as the most nota...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  Sweet United Nations video. Just in time for C...      1\n",
       "1  @mrdahl87 We are rumored to have talked to Erv...      1\n",
       "2  Hey there! Nice to see you Minnesota/ND Winter...      1\n",
       "3                3 episodes left I'm dying over here      0\n",
       "4  \"I can't breathe!\" was chosen as the most nota...      1"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train Data\n",
    "\n",
    "data, y_label = parse_dataset('SemEval2018-T3-train-taskA_emoji.txt')\n",
    "data_df = pd.DataFrame(np.array(data).reshape(3834,1), columns = ['tweet'])\n",
    "data_df['label'] = np.array(y_label).reshape(3834,1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 784 entries, 0 to 783\n",
      "Data columns (total 2 columns):\n",
      "tweet    784 non-null object\n",
      "label    784 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 12.4+ KB\n"
     ]
    }
   ],
   "source": [
    "### Test Data\n",
    "\n",
    "# Test the model\n",
    "test_data, test_label = parse_dataset('SemEval2018-T3_gold_test_taskA_emoji.txt')\n",
    "test_df = pd.DataFrame(np.array(test_data).reshape(784,1), columns = ['tweet'])\n",
    "test_df['label'] = np.array(test_label).reshape(784,1)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plotCM(cm):\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt=\"d\"); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(['Irony', 'Non-Irony']); ax.yaxis.set_ticklabels(['Irony', 'Non-Irony']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalysis(sentencePart, feature):\n",
    "\n",
    "    \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    tweet_len = len(sentencePart.split(' '))\n",
    "\n",
    "    pos_word_list=[]\n",
    "    neu_word_list=[]\n",
    "    neg_word_list=[]\n",
    "    highest_pos = float('-inf')\n",
    "    lowest_neg = float('inf')\n",
    "    emoji=[]\n",
    "    negate_flag = False\n",
    "    total_polarity = 0\n",
    "\n",
    "#     threshold = -0.3 -> 0.3\n",
    "    for word in sentencePart.split(' '):\n",
    "        hashtag_flag = False\n",
    "\n",
    "#         do some preprocessing\n",
    "        if(word.startswith('#')):\n",
    "            hashtag_flag = True\n",
    "            word = word.replace('#', '')\n",
    "        \n",
    "        comp_polarity = sia.polarity_scores(word)['compound']\n",
    "        if(hashtag_flag):\n",
    "            comp_polarity = 1.2 * comp_polarity\n",
    "        if(negate_flag):\n",
    "            comp_polarity = -(comp_polarity)\n",
    "        total_polarity += comp_polarity\n",
    "        \n",
    "#         if word = 'not' implies negate flag till punctuation\n",
    "#         if (word in NEGATE):\n",
    "#             negate_flag = True\n",
    "            \n",
    "#         if(word in string.punctuation):\n",
    "#             negate_flag = False\n",
    "            \n",
    "        if (comp_polarity) >= 0.3 or word in HAPPY:\n",
    "            if(comp_polarity > highest_pos):\n",
    "                highest_pos = comp_polarity\n",
    "            pos_word_list.append(word)\n",
    "        \n",
    "        elif (comp_polarity) <= -0.3 or word in NEGATE or word in SAD:\n",
    "            if(comp_polarity < lowest_neg):\n",
    "                lowest_neg = comp_polarity\n",
    "            neg_word_list.append(word)\n",
    "        else:\n",
    "            neu_word_list.append(word)\n",
    "    \n",
    "    if(feature == 'avg'):\n",
    "        return [len(pos_word_list)/tweet_len, len(neu_word_list)/tweet_len, len(neg_word_list)/tweet_len]\n",
    "    if(feature == 'overall'):\n",
    "        return total_polarity/tweet_len\n",
    "    if(feature == 'diff'):\n",
    "        diff = highest_pos - lowest_neg\n",
    "        if(diff == float('-inf') or diff == float('inf')):\n",
    "            return 0\n",
    "        else:\n",
    "            return diff\n",
    "        return highest_pos - lowest_neg\n",
    "    if(feature == 'inversion'):\n",
    "        return len(pos_word_list) and len(neg_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature 1 - number of Named Entity in a tweet using NLTK \n",
    "\n",
    "#The below code identiies and counts the named entities (proper nouns) in tweets using nltk tagset\n",
    "#as nltk tags named entities as a POS, hence the job is to identify NE POS tag\n",
    "\n",
    "\n",
    "#this function creates an NLTK POS tree of each tokenized sentence\n",
    "\n",
    "def named_entity_preprocess(data):\n",
    "    sentences = nltk.sent_tokenize(data)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "    return chunked_sentences\n",
    "\n",
    "#identify whether feature is NE or not\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "    if hasattr(t, 'label') and t.label():\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "#counting of NE via loop and then normalizing it for each tweet\n",
    "def named_entity_count(tweet):\n",
    "#     named_entity_list = []\n",
    "      \n",
    "#     for tweet in sample:\n",
    "    chunked_sentences = named_entity_preprocess(tweet)\n",
    "    entity_names = []\n",
    "    for tree in chunked_sentences:\n",
    "        entity_names.extend(extract_entity_names(tree))\n",
    "    ne_count = len(entity_names)\n",
    "    word_count = len(tweet.split())\n",
    "    ne_score = ne_count / word_count\n",
    "#     named_entity_list.append(ne_score)\n",
    "    return ne_score\n",
    "\n",
    "#Feature 2 - number of adverbs\n",
    "\n",
    "def adv_counter(tweet): \n",
    "#     adv_list = []\n",
    "#     for tweet in tweets:\n",
    "\n",
    "    tagged = pos_tag(word_tokenize(tweet))\n",
    "    word_count = 0\n",
    "    adv_count = 0\n",
    "    adv_score = 0\n",
    "\n",
    "    for tag in tagged:         \n",
    "\n",
    "        if (tag[1] == 'RB') or (tag[1] == 'RBS') or (tag[1] == 'RBR'):\n",
    "            adv_count += 1\n",
    "\n",
    "        word_count += 1\n",
    "\n",
    "    adv_score = adv_count / word_count\n",
    "#     adv_list.append(adv_score)\n",
    "\n",
    "    return adv_score\n",
    "\n",
    "#Feature 3 - number of adjectives\n",
    "\n",
    "def adj_counter(tweet):\n",
    "#     adj_list = []\n",
    "     \n",
    "#     for tweet in tweets:\n",
    "            \n",
    "    tagged = pos_tag(word_tokenize(tweet))\n",
    "    word_count = 0\n",
    "    adj_count = 0\n",
    "    adj_score = 0\n",
    "\n",
    "    for tag in tagged:\n",
    "        if (tag[1] == 'JJ') or (tag[1] == 'JJR') or (tag[1] == 'JJS'):\n",
    "                adj_count += 1\n",
    "\n",
    "        word_count += 1\n",
    "    adj_score = adj_count / word_count\n",
    "#     adj_list.append(adj_score)\n",
    "        \n",
    "    return adj_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hashtag_emoji(tweet):\n",
    "    tweet = re.sub(r'\\B#','', tweet)\n",
    "    tweet = emoji.demojize(tweet, delimiters=(\"\", \"\"))\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "count_punc = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "\n",
    "def combined_feature_extract(og_df):\n",
    "    df = og_df.copy()\n",
    "#     Lexical Features\n",
    "    df['capitalisation'] = df['tweet'].apply(lambda x: len(list(ch for ch in x if ch.isupper() == 1))/len(x))\n",
    "#     df['url'] = df['tweet'].apply(lambda x: 1 if len(re.findall(r'http\\S+', x)) else 0)\n",
    "    df['hashtag_counts'] = df['tweet'].apply(lambda x: len(re.findall(r'\\B#\\w*[a-zA-Z]+\\w*', x)))\n",
    "#     df['emoji_counts'] = df['tweet'].apply(lambda x: len(demoji.findall(row['tweet'])))\n",
    "#     df['@_counts'] = df['tweet'].apply(lambda x: len(re.findall(r'\\B@\\w*[a-zA-Z]+\\w*', x)))\n",
    "    df['length'] = df['tweet'].apply(lambda x: len(x.split(' ')))\n",
    "    df['hashtag_word_ratio'] = df['hashtag_counts']/df['length']\n",
    "    df['# punctuation'] = df['tweet'].apply(lambda x: count_punc(x, string.punctuation))\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: replace_hashtag_emoji(x)) #creating a new column\n",
    "\n",
    "#   Sentiment features\n",
    "    df[['positive_avg','neutral_avg', 'negative_avg']] = df['tweet'].apply(lambda x: pd.Series(sentimentAnalysis(x, feature = 'avg')))\n",
    "    df['overall'] = df['tweet'].apply(lambda x: sentimentAnalysis(x, feature = 'overall'))\n",
    "    df['ne_score'] = df['tweet'].apply(lambda x: named_entity_count(x))\n",
    "    df['adv_score'] = df['tweet'].apply(lambda x: adv_counter(x))\n",
    "    df['adj_score'] = df['tweet'].apply(lambda x: adj_counter(x))\n",
    "\n",
    "    return df.drop(['tweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_feature_extract(data_df)\n",
    "y_train = combined_df['label']\n",
    "combined_df = combined_df.drop(['label', 'hashtag_counts'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capitalisation</th>\n",
       "      <th>length</th>\n",
       "      <th>hashtag_word_ratio</th>\n",
       "      <th># punctuation</th>\n",
       "      <th>positive_avg</th>\n",
       "      <th>neutral_avg</th>\n",
       "      <th>negative_avg</th>\n",
       "      <th>overall</th>\n",
       "      <th>ne_score</th>\n",
       "      <th>adv_score</th>\n",
       "      <th>adj_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>9</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067715</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.009429</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3829</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3830</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3831</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>19</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3832</td>\n",
       "      <td>0.095588</td>\n",
       "      <td>18</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>21</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3833</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>21</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3834 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      capitalisation  length  hashtag_word_ratio  # punctuation  positive_avg  \\\n",
       "0           0.110000      13            0.153846              9      0.153846   \n",
       "1           0.039683      24            0.000000             11      0.041667   \n",
       "2           0.129630       9            0.000000              2      0.111111   \n",
       "3           0.028571       7            0.000000              1      0.000000   \n",
       "4           0.024000      23            0.000000              4      0.000000   \n",
       "...              ...     ...                 ...            ...           ...   \n",
       "3829        0.061224       7            0.000000              1      0.000000   \n",
       "3830        0.030769      12            0.000000              2      0.000000   \n",
       "3831        0.029412      19            0.052632              4      0.000000   \n",
       "3832        0.095588      18            0.444444             21      0.055556   \n",
       "3833        0.039216      21            0.047619              5      0.000000   \n",
       "\n",
       "      neutral_avg  negative_avg   overall  ne_score  adv_score  adj_score  \n",
       "0        0.846154      0.000000  0.067715  0.166667   0.062500   0.000000  \n",
       "1        0.916667      0.041667  0.009429  0.095238   0.037037   0.037037  \n",
       "2        0.888889      0.000000  0.046833  0.000000   0.000000   0.000000  \n",
       "3        1.000000      0.000000  0.000000  0.000000   0.125000   0.000000  \n",
       "4        0.956522      0.043478  0.000000  0.043478   0.074074   0.074074  \n",
       "...           ...           ...       ...       ...        ...        ...  \n",
       "3829     1.000000      0.000000  0.000000  0.142857   0.000000   0.125000  \n",
       "3830     1.000000      0.000000 -0.016858  0.000000   0.142857   0.071429  \n",
       "3831     1.000000      0.000000  0.014379  0.000000   0.136364   0.045455  \n",
       "3832     0.944444      0.000000  0.017678  0.000000   0.000000   0.000000  \n",
       "3833     1.000000      0.000000 -0.010419  0.000000   0.090909   0.045455  \n",
       "\n",
       "[3834 rows x 11 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined_df = combined_feature_extract(test_df)\n",
    "y_test = test_combined_df['label']\n",
    "test_combined_df = test_combined_df.drop(['label', 'hashtag_counts'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.1}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_param_selection(combined_df, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6122448979591837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7107    0.6025    0.6522       473\n",
      "           1     0.5091    0.6270    0.5620       311\n",
      "\n",
      "    accuracy                         0.6122       784\n",
      "   macro avg     0.6099    0.6148    0.6071       784\n",
      "weighted avg     0.6308    0.6122    0.6164       784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = svm.SVC(C=10, gamma = 0.1, kernel = 'rbf')\n",
    "clf.fit(combined_df, y_train)\n",
    "y_pred = clf.predict(test_combined_df)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "svm_cm = confusion_matrix(y_test, y_pred)\n",
    "plotCM(svm_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROSS VALIDATION\n",
      "Score: [0.58441558 0.61979167 0.63020833 0.60052219 0.5848564  0.63968668\n",
      " 0.58224543 0.61879896 0.5848564  0.54569191]\n",
      "Mean: 0.599107354786206\n",
      "std: 0.026816257884689297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6895    0.6808    0.6851       473\n",
      "           1     0.5237    0.5338    0.5287       311\n",
      "\n",
      "    accuracy                         0.6224       784\n",
      "   macro avg     0.6066    0.6073    0.6069       784\n",
      "weighted avg     0.6237    0.6224    0.6230       784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross-validation on train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Example of cross-validation\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "score = cross_val_score(rf, combined_df, y_train, cv=10, scoring = \"accuracy\")\n",
    "print('CROSS VALIDATION')\n",
    "print(\"Score:\", score)\n",
    "print(\"Mean:\", score.mean())\n",
    "print(\"std:\", score.std())\n",
    "\n",
    "rf.fit(combined_df, y_train)\n",
    "y_pred = rf.predict(test_combined_df)\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('capitalisation', 0.03963061810719428)\n",
      "('length', 0.056699918502077035)\n",
      "('hashtag_word_ratio', 0.06159828508858917)\n",
      "('# punctuation', 0.06930516625606536)\n",
      "('positive_avg', 0.07807462278436059)\n",
      "('neutral_avg', 0.0853583181136645)\n",
      "('negative_avg', 0.09911111049943701)\n",
      "('overall', 0.09952418480322398)\n",
      "('ne_score', 0.11551122228172492)\n",
      "('adv_score', 0.13486413295632058)\n",
      "('adj_score', 0.16032242060734275)\n"
     ]
    }
   ],
   "source": [
    "feat_labels = list(combined_df)\n",
    "for feature in zip(feat_labels, sorted(rf.feature_importances_)):\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected bytes, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-4d8a29cf1e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected bytes, int found"
     ]
    }
   ],
   "source": [
    "np.sort(1,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
